{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a13a391",
   "metadata": {},
   "source": [
    "choose the sft_env(python3.10.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bc6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (4.55.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998de7d9",
   "metadata": {},
   "source": [
    "# This explicitly uses the pip associated with Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f27ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # PyTorch library\n",
    "from transformers import pipeline # Hugging Face Transformers library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62fe03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0, dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Check for MPS, then CUDA, else CPU\n",
    "dtype = torch.float16 if device == \"mps\" else torch.float32 # Use float16 for MPS, else float32\n",
    "print(f\"Using device: {device}, dtype: {dtype}\") # Print the selected device and data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba98ee6",
   "metadata": {},
   "source": [
    "MPS stands for Metal Performance Shaders, which is a framework developed by Apple to accelerate GPU computations on macOS and iOS devices. In the context of PyTorch, MPS allows you to run tensor operations and deep learning models on Apple GPUs (like those in M1, M2, or M3 chips), offering a hardware-accelerated alternative to CPU execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788ec7e",
   "metadata": {},
   "source": [
    "Key Points about MPS in PyTorch:\n",
    "* Platform-specific: Only available on macOS with Apple Silicon or supported Intel Macs.\n",
    "* Alternative to CUDA: CUDA is for NVIDIA GPUs, while MPS is for Apple GPUs.\n",
    "* Improves performance: Using MPS can significantly speed up training and inference compared to CPU.\n",
    "* Supported in PyTorch: Starting from PyTorch 1.12, MPS support was introduced experimentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2346ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2208bf1a",
   "metadata": {},
   "source": [
    "XetHub is a Git-based data versioning system that Hugging Face integrates to improve performance when accessing large models or datasets. It allows faster and more efficient downloads by using a specialized backend.\n",
    "\n",
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67027143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c42329134c46f89f228282de05fa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c1ce851b374b668980e246033ccdeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:  29%|##8       | 629M/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8e823d5c1148d382ccdd94fb63d31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  18%|#8        | 734M/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ch939\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3024a10d4ea44d748b2015e33f4a4fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db670c3749c424b8cc7fbf33f4e31d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35d2ca456804bb49b37a9a8bae5486f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7158d3a8c58f4fd68045396ae6e78686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d328babf3c248578bc468d483b68ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5938c52dba134cc59d3bd2f8900c9115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Scott Lai? Scott Lai, a native of Taiwan, is an accomplished entrepreneur and investor who has been making waves in the tech industry. Here are some key points about him:\n",
      "\n",
      "1. Founder: He is the founder of Poshmark, a leading online marketplace for second-hand clothing.\n",
      "\n",
      "2. Early Career: Before founding Poshmark, Lai worked as a software engineer at Google and eBay.\n",
      "\n",
      "3. Poshmark: Founded in 2010, Poshmark revolutionized the second-hand clothing market by creating a user-friendly platform where buyers and sellers could easily exchange clothes. The company grew rapidly and went public in 2019.\n",
      "\n",
      "4. Other Ventures: In addition to Poshmark, Lai has invested in several other startups, including Rent the Runway and Trunk Club.\n",
      "\n",
      "5. Philanthropy: He is involved in various charitable causes, particularly focusing on education and entrepreneurship.\n",
      "\n",
      "6. Leadership: Lai has been recognized for his leadership skills and has spoken at numerous conferences and events.\n",
      "\n",
      "7. Entrepreneurial Spirit: His background as a software engineer and his experiences at Google and eBay have contributed to his success in launching successful tech businesses.\n",
      "\n",
      "8. Industry Recognition: Poshmark's success has earned Lai recognition within the tech industry, with\n"
     ]
    }
   ],
   "source": [
    "ask_llm = pipeline( # Initialize the text generation pipeline\n",
    "  task=\"text-generation\", # Specify the task as text generation\n",
    "  model=\"Qwen/Qwen2.5-3B-Instruct\", # You can replace this with another model if desired\n",
    "  device=device, # Use the selected device\n",
    "  torch_dtype=dtype # Use the selected data type\n",
    ")\n",
    "\n",
    "print(ask_llm(\"Who is Scott Lai?\")[0][\"generated_text\"]) # Generate text based on the prompt and print the result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145dc2e4",
   "metadata": {},
   "source": [
    "As you can see here, the model has no idea who I am from above response.\n",
    "\n",
    "Let's cook it!\n",
    "\n",
    "First, let's teach the model who I am. Here you can use your personal data to generate the exact format you will use for fine-turning base on your own data. You can use ChatGPT for this, just ask it to transfer your resume into the trainable json format with \"prompt\" and \"completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b8a7c33b594d7382059e58c2fb90c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 122\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "from datasets import load_dataset # Hugging Face Datasets library\n",
    "\n",
    "raw_data = load_dataset('json', data_files = \"scott_lai_resume_train.json\") # Load the dataset from a JSON file\n",
    "raw_data # Print the loaded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3bb96",
   "metadata": {},
   "source": [
    "The Hugging Face load_dataset function is automatically creating a default split named \"train\" when you load a dataset from a local JSON file and don’t explicitly specify any splits.\n",
    "The load_dataset function expects datasets to be split into subsets like \"train\", \"test\", \"validation\", etc.\n",
    "Since your JSON file doesn’t specify any splits, Hugging Face assumes the entire dataset is for training and labels it as \"train\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is Scott Lai’s profession?',\n",
       " 'completion': 'AI Engineer and Data Scientist.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"train\"][0] # Access the first entry in the training split of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13cf6e",
   "metadata": {},
   "source": [
    "As you can see, here we return with the long text, but for fine-tuning we need the data to be small and precise chunks, more like here we apply the tokenization to take the text and split it into smaller chunks. Each chunk is called a token and it the smallest unit of meaning that LLMs work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da03887b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a80ed1167804b9fb13e71d030c4e9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Scott Lai’s profession?\n",
      "AI Engineer and Data Scientist.\n",
      "How many years of experience does Scott Lai have in generative AI and LLM solutions?\n",
      "Over 5 years.\n",
      "What infrastructures is Scott Lai skilled in designing and optimizing?\n",
      "Scalable ML infrastructures using PyTorch, Hugging Face, and FastAPI on AWS.\n",
      "What type of workflows is Scott Lai experienced in building?\n",
      "End-to-end pipelines, scalable microservices, and ETL workflows.\n",
      "What collaboration experience does Scott Lai have?\n",
      "Proven track record in cross-functional collaboration and implementing ML and data engineering best practices.\n",
      "Which skill in Programming & Scripting does Scott Lai have?\n",
      "Python\n",
      "Which skill in Programming & Scripting does Scott Lai have?\n",
      "Rust\n",
      "Which skill in Programming & Scripting does Scott Lai have?\n",
      "Node.js\n",
      "Which skill in Programming & Scripting does Scott Lai have?\n",
      "HTML\n",
      "Which skill in Programming & Scripting does Scott Lai have?\n",
      "CSS\n",
      "Which skill in Programming & Scripting does Scott Lai have?\n",
      "JavaScript\n",
      "Which skill in Programming & Scripting does Scott Lai have?\n",
      "R\n",
      "Which skill in Programming & Scripting does Scott Lai have?\n",
      "Bash\n",
      "Which skill in Programming & Scripting does Scott Lai have?\n",
      "GoLang\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "Machine Learning\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "Deep Learning\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "Reinforcement Learning\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "TensorFlow\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "PyTorch\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "Scikit Learn\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "Hugging Face\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "LangChain\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "LlamaIndex\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "Generative AI\n",
      "Which skill in Machine Learning & AI does Scott Lai have?\n",
      "LangGraph\n",
      "Which skill in ML Infrastructure & Optimization does Scott Lai have?\n",
      "LLM Serving Frameworks\n",
      "Which skill in ML Infrastructure & Optimization does Scott Lai have?\n",
      "ETL Pipelines\n",
      "Which skill in ML Infrastructure & Optimization does Scott Lai have?\n",
      "TensorRT\n",
      "Which skill in ML Infrastructure & Optimization does Scott Lai have?\n",
      "Deepspeed\n",
      "Which skill in Cloud & DevOps does Scott Lai have?\n",
      "AWS\n",
      "Which skill in Cloud & DevOps does Scott Lai have?\n",
      "GCP\n",
      "Which skill in Cloud & DevOps does Scott Lai have?\n",
      "Azure\n",
      "Which skill in Cloud & DevOps does Scott Lai have?\n",
      "Docker\n",
      "Which skill in Cloud & DevOps does Scott Lai have?\n",
      "Kubernetes\n",
      "Which skill in Cloud & DevOps does Scott Lai have?\n",
      "Beanstalk\n",
      "Which skill in Cloud & DevOps does Scott Lai have?\n",
      "Terraform\n",
      "Which skill in Cloud & DevOps does Scott Lai have?\n",
      "S3\n",
      "Which skill in Cloud & DevOps does Scott Lai have?\n",
      "EC2\n",
      "Which skill in Web & API Development does Scott Lai have?\n",
      "Flask\n",
      "Which skill in Web & API Development does Scott Lai have?\n",
      "Django\n",
      "Which skill in Web & API Development does Scott Lai have?\n",
      "FastAPI\n",
      "Which skill in Web & API Development does Scott Lai have?\n",
      "API Design\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Git\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "CI/CD\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Unit Testing\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Integration Testing\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "DataDog\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Tableau\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Spark\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Hadoop\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Stable Diffusion\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Claude code\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Midjourney\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "CodeWhisperer\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "OCR\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Cursor\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "taskMaster\n",
      "Which skill in Tools & Methodologies does Scott Lai have?\n",
      "Computer Vision\n",
      "What did Scott Lai accomplish at BlueberryAI?\n",
      "Developed production-ready AI-driven agents and advanced RAG models for legal analysis on AWS.\n",
      "What did Scott Lai accomplish at BlueberryAI?\n",
      "Engineered and optimized data pipelines using AWS and Databricks.\n",
      "What did Scott Lai accomplish at BlueberryAI?\n",
      "Collaborated with cross-functional teams to address legal and regulatory compliance.\n",
      "What did Scott Lai accomplish at BlueberryAI?\n",
      "Communicated model metrics clearly to stakeholders.\n",
      "What did Scott Lai accomplish at BlueberryAI?\n",
      "Enhanced AI models for legal identification by integrating real-time analysis features into client-facing apps.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Designed and deployed a production-grade RAG system using LangChain and AWS DynamoDB.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Improved search accuracy by 35% through RAG optimization.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Engineered an AI-driven document processing pipeline handling 10,000+ daily requests with 98% accuracy.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Applied OCR techniques with optimized Python solutions.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Optimized custom LLM models achieving 17% better performance than ChatGPT and Claude 3.5.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Implemented a distributed multi-agent LLM architecture with FastAPI and React.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Supported over 500 concurrent users with the system.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Constructed scalable ETL pipelines with Apache Spark and Airflow.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Processed over 2TB of data efficiently in ScholarAI pipelines.\n",
      "What did Scott Lai contribute at ScholarAI?\n",
      "Integrated CI/CD best practices into ML model operations.\n",
      "What achievement did Scott Lai have at Ember Learning?\n",
      "Developed DeapLearning, an AI-powered tutoring platform with neural TTS.\n",
      "What achievement did Scott Lai have at Ember Learning?\n",
      "Served over 120,000 K-12 learners with DeapLearning.\n",
      "What achievement did Scott Lai have at Ember Learning?\n",
      "Directed a 13-person team across engineering, ML, and education.\n",
      "What achievement did Scott Lai have at Ember Learning?\n",
      "Launched a workshop builder and rubric grader reducing teacher workload by 60%.\n",
      "What achievement did Scott Lai have at Ember Learning?\n",
      "Architected a FERPA-secure LangChain-GPT-40 stack on AWS for 500+ concurrent users.\n",
      "What achievement did Scott Lai have at Ember Learning?\n",
      "Enhanced customer satisfaction through RLHF tuning.\n",
      "What achievement did Scott Lai have at Ember Learning?\n",
      "Expanded AP course offerings with reinforcement learning from human feedback.\n",
      "What achievement did Scott Lai have at Ember Learning?\n",
      "Formulated presentations that secured $1.2M in seed funding.\n",
      "What did Scott Lai do at Duke University?\n",
      "Collaborated with Hugging Face on LLM development using Rust Candle.\n",
      "What did Scott Lai do at Duke University?\n",
      "Managed AWS and Azure cloud research projects.\n",
      "What did Scott Lai do at Duke University?\n",
      "Led AWS Cloud Club with over 200 students.\n",
      "What did Scott Lai do at Duke University?\n",
      "Designed technical curricula for advanced AI and software development.\n",
      "What did Scott Lai do at Duke University?\n",
      "Provided mentorship to students in ML and scalable system design.\n",
      "What was Scott Lai’s contribution at Spigot, Inc.?\n",
      "Developed AI tools using ML and DL to enhance company operations.\n",
      "What was Scott Lai’s contribution at Spigot, Inc.?\n",
      "Conducted Big Data research with R for insights and trends.\n",
      "What was Scott Lai’s contribution at Spigot, Inc.?\n",
      "Explored Blockchain applications in FinTech.\n",
      "What was Scott Lai’s contribution at Spigot, Inc.?\n",
      "Enhanced transactional security with Blockchain solutions.\n",
      "What was Scott Lai’s contribution at Spigot, Inc.?\n",
      "Collaborated on FinTech projects with scalable ML methodologies.\n",
      "What did Scott Lai achieve at IUNISPACE?\n",
      "Led a data analysis team designing 12 quantitative trading algorithms.\n",
      "What did Scott Lai achieve at IUNISPACE?\n",
      "Helped the firm earn $2M in China's A-share market with ML trading strategies.\n",
      "What did Scott Lai achieve at IUNISPACE?\n",
      "Designed and deployed three data intelligence platforms for banking, real estate, and distressed assets.\n",
      "What did Scott Lai achieve at IUNISPACE?\n",
      "Saved clients $50M in labor and efficiency costs across 7+ corporations.\n",
      "What did Scott Lai achieve at IUNISPACE?\n",
      "Led simulation analysis of satellite remote sensing data for rocket and satellite launch.\n",
      "What did Scott Lai achieve at IUNISPACE?\n",
      "Integrated satellite imagery, GIS, and mobility data with financial analytics.\n",
      "What did Scott Lai achieve at IUNISPACE?\n",
      "Enhanced trading strategies with alternative data sources.\n",
      "What did Scott Lai achieve at IUNISPACE?\n",
      "Directed multidisciplinary teams in ML-driven solutions.\n",
      "What accomplishment did Scott Lai make at J.P. Morgan?\n",
      "Built predictive analytics models reducing loan default rates by 13%.\n",
      "What accomplishment did Scott Lai make at J.P. Morgan?\n",
      "Engineered scalable ETL pipelines and automated reporting systems.\n",
      "What accomplishment did Scott Lai make at J.P. Morgan?\n",
      "Implemented NLP solutions for transaction monitoring and fraud detection.\n",
      "What accomplishment did Scott Lai make at J.P. Morgan?\n",
      "Conducted analysis on customer behavior and transaction data.\n",
      "What accomplishment did Scott Lai make at J.P. Morgan?\n",
      "Presented technical findings to leadership with actionable recommendations.\n",
      "What is Scott Lai’s educational background?\n",
      "Earned a Master of Science in Interdisciplinary Data Science from Duke University.\n",
      "What is Scott Lai’s educational background?\n",
      "Studied Statistical Modeling, Data Engineering, NLP, Data Analysis in Cloud, and Machine Learning at Duke.\n",
      "What is Scott Lai’s educational background?\n",
      "Earned a Bachelor of Science in Statistics and Economics from University of Wisconsin-Madison.\n",
      "What is Scott Lai’s educational background?\n",
      "Studied Calculus, Linear Regression, Data Visualization, and Machine Learning in Python at UW-Madison.\n",
      "What is a lesson or contribution from Scott Lai’s ScholarAI patent project?\n",
      "Developed an AI-powered legal assistant for patent prosecution with Wilson Sonsini.\n",
      "What is a lesson or contribution from Scott Lai’s ScholarAI patent project?\n",
      "Built multi-agent system using FastAPI and LangChain.\n",
      "What is a lesson or contribution from Scott Lai’s ScholarAI patent project?\n",
      "Integrated USPTO data for claim drafting and prior art analysis.\n",
      "What is a lesson or contribution from Scott Lai’s ScholarAI patent project?\n",
      "Implemented backend handling 10,000+ documents daily with 98% accuracy.\n",
      "What is a lesson or contribution from Scott Lai’s ScholarAI patent project?\n",
      "Designed 4-panel interface mimicking attorney workflows.\n",
      "What is a lesson or contribution from Scott Lai’s ScholarAI patent project?\n",
      "Learned that overfitting to single-user workflows reduces adoption.\n",
      "What is a lesson or contribution from Scott Lai’s ScholarAI patent project?\n",
      "Recognized need for model tuning for domain-specific legal writing styles.\n",
      "What is a lesson or contribution from Scott Lai’s ScholarAI patent project?\n",
      "Identified trust deficit due to hallucinations and lack of explainability.\n",
      "What is a lesson or contribution from Scott Lai’s ScholarAI patent project?\n",
      "Gained lessons in human-AI interface design and iteration from feedback.\n",
      "Which cloud platforms is Scott Lai experienced with?\n",
      "AWS, GCP, and Azure.\n",
      "Which containerization and orchestration tools does Scott Lai use?\n",
      "Docker and Kubernetes.\n",
      "Which visualization tools has Scott Lai worked with?\n",
      "Tableau and Hadoop ecosystem tools.\n",
      "Which generative image models has Scott Lai used?\n",
      "Stable Diffusion and Midjourney.\n",
      "What programming languages does Scott Lai know besides Python?\n",
      "Rust, Node.js, JavaScript, R, Bash, GoLang, HTML, and CSS.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # Import the AutoTokenizer class from the Transformers library\n",
    "# Load the tokenizer for the specified model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\"\n",
    ")\n",
    "def preprocess(sample): # Define a function to preprocess each sample in the dataset\n",
    "    sample = sample['prompt']+ '\\n' + sample['completion'] # Concatenate the prompt and completion with a newline\n",
    "    print(sample)\n",
    "    tokenized = tokenizer( # Tokenize the sample\n",
    "        sample, # Use the tokenizer to convert the text into tokens\n",
    "        max_length = 128, # Set the maximum length of the tokenized sequence\n",
    "        truncation = True, # Truncate sequences longer than the maximum length\n",
    "        padding = \"max_length\"  # Pad sequences to the maximum length  \n",
    "    )\n",
    "\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy() # Create a copy of the input IDs for the labels\n",
    "    return tokenized # Return the tokenized sample\n",
    "data = raw_data.map(preprocess) # Apply the preprocessing function to the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3efa2d7",
   "metadata": {},
   "source": [
    "Qwen2.5-3B-Instruct is a compact yet powerful instruction-tuned language model developed by Alibaba Cloud as part of the Qwen2.5 series. Here's a detailed overview of its features and capabilities:\n",
    "Model Type: Causal Language Model (decoder-only)\n",
    "Architecture:\n",
    "* Transformers with RoPE (Rotary Position Embedding)\n",
    "* SwiGLU activation\n",
    "* RMSNorm normalization\n",
    "* Attention with QKV bias\n",
    "* Tied word embeddings\n",
    "Parameters:\n",
    "* Total: 3.09 billion\n",
    "* Non-embedding: 2.77 billion\n",
    "Layers: 36\n",
    "Attention Heads: 16 for queries, 2 for keys/values (Grouped-Query Attention)\n",
    "Context Length:\n",
    "* Input: up to 32,768 tokens\n",
    "* Output generation: up to 8,192 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc795915",
   "metadata": {},
   "source": [
    "The tokenization is done pair by pair. The preprocess function is applied to each individual sample (each prompt-completion pair) in the dataset via dataset.map(), meaning each pair is tokenized separately, though efficiently and quickly due to internal optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f661a8",
   "metadata": {},
   "source": [
    "tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "This is for training language models, where the model tries to predict the next token. So the labels are set to be the same as input_ids.\n",
    " In causal language modeling (like with Qwen), we train the model to reconstruct the input — so input = label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6aae75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 122\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2dd823",
   "metadata": {},
   "source": [
    "When you pass text to a Hugging Face tokenizer, it returns a dictionary containing:\n",
    "\n",
    "* input_ids: The tokenized text converted into numerical IDs (each word/subword → ID).\n",
    "* attention_mask: A binary mask indicating which tokens are real (1) vs. padded (0), so the model knows to ignore padding.\n",
    "* (Optional) token_type_ids: Not shown here — used for sentence-pair tasks like NLI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915453d5",
   "metadata": {},
   "source": [
    "Print one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df5e322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [3838, 374, 9815, 444, 2143, 748, 4808, 5267, 15469, 28383, 323, 2885, 67309, 13, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels: [3838, 374, 9815, 444, 2143, 748, 4808, 5267, 15469, 28383, 323, 2885, 67309, 13, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]\n"
     ]
    }
   ],
   "source": [
    "# Access the first sample in the dataset\n",
    "sample = data['train'][0]\n",
    "print(\"Input IDs:\", sample['input_ids'])\n",
    "print(\"Attention Mask:\", sample['attention_mask'])\n",
    "print(\"Labels:\", sample['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6f17b",
   "metadata": {},
   "source": [
    "Print shape or length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ac86b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids: 128\n",
      "Number of non-padded tokens: 14\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of input_ids:\", len(sample['input_ids']))  # Should be 128 (due to max_length)\n",
    "print(\"Number of non-padded tokens:\", sum(sample['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6530241",
   "metadata": {},
   "source": [
    "Decode input_ids back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1194d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: What is Scott Lai’s profession?\n",
      "AI Engineer and Data Scientist.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# See what the tokens actually represent\n",
    "decoded = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
    "print(\"Decoded text:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c5fe0",
   "metadata": {},
   "source": [
    "The integer token IDs themselves are not encoded using variable-length codes like Huffman coding — at least not during normal model training/inference in Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ab97f",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "now, let's move into the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90228ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType # PEFT library for parameter-efficient fine-tuning\n",
    "from transformers import AutoModelForCausalLM # Import the AutoModelForCausalLM class from the Transformers library\n",
    "import torch # PyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7c30e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab8882a9c1e44f985dc48391793149b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,506,752 || all params: 3,088,445,440 || trainable%: 0.0812\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained( # Load the pre-trained model\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    device_map = device, # Use the selected device\n",
    "    torch_dtype = torch.float16 # Use float16 data type\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig (\n",
    "    \n",
    "    task_type = TaskType.CAUSAL_LM,  # Specify the task type as causal language modeling\n",
    "    target_modules=['q_proj', \"k_proj\", \"v_proj\"] # Target specific modules for LoRA adaptation\n",
    ")\n",
    "model = get_peft_model(model, lora_config) # Apply the LoRA configuration to the model\n",
    "model.print_trainable_parameters() # Print the number of trainable parameters in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33200b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer # Import the TrainingArguments and Trainer classes from the Transformers library\n",
    "\n",
    "\n",
    "train_args = TrainingArguments( # Define training arguments\n",
    "    num_train_epochs = 10, # we will go throught the dataset from start to finish 10 times\n",
    "    learning_rate=0.001, # learning rate for the optimizer\n",
    "    logging_steps = 25, # we want to see the result in every 25 steps it runs \n",
    "    fp16 = True # float point set to 16 to speed it up, set to \"True\" if you are on GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args = train_args,\n",
    "    model = model, \n",
    "    train_dataset=data[\"train\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b17f16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 31:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.131500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.8151650987565517, metrics={'train_runtime': 1887.9895, 'train_samples_per_second': 0.646, 'train_steps_per_second': 0.085, 'total_flos': 2602200748523520.0, 'train_loss': 0.8151650987565517, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba2561",
   "metadata": {},
   "source": [
    "It takes 31 m 28.5 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed1aafcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my-qwen\\\\tokenizer_config.json',\n",
       " './my-qwen\\\\special_tokens_map.json',\n",
       " './my-qwen\\\\chat_template.jinja',\n",
       " './my-qwen\\\\vocab.json',\n",
       " './my-qwen\\\\merges.txt',\n",
       " './my-qwen\\\\added_tokens.json',\n",
       " './my-qwen\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "trainer.save_model(\"./my-qwen\")\n",
    "tokenizer.save_pretrained(\"./my-qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7449af4",
   "metadata": {},
   "source": [
    "Now let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71e8b4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b175484e92d544f1ae4b6fb6e95400e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 25.76 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ask_llm \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./my-qwen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./my-qwen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use the selected data type\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(ask_llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho is Scott Lai?\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1210\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1208\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[1;32m-> 1210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline_class(model\u001b[38;5;241m=\u001b[39mmodel, framework\u001b[38;5;241m=\u001b[39mframework, task\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:121\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[0;32m    123\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[0;32m    124\u001b[0m     )\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\transformers\\pipelines\\base.py:1043\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m ):\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If it's a generation pipeline and the model can generate:\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# tweaks to the generation config.\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# 2 - load the assistant model if it is passed.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_calls_generate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate():\n",
      "File \u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\transformers\\modeling_utils.py:4341\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   4337\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4338\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4339\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4340\u001b[0m         )\n\u001b[1;32m-> 4341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 25.76 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "ask_llm = pipeline(\n",
    "  task=\"text-generation\",\n",
    "  model=\"./my-qwen\",\n",
    "  tokenizer='./my-qwen',\n",
    "  device=device,\n",
    "  torch_dtype=dtype, # Use the selected data type\n",
    ")\n",
    "\n",
    "print(ask_llm(\"Who is Scott Lai?\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef1abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sft_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
