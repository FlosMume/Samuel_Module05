{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Embedding Database Optimization\n",
    "\n",
    "This weekâ€™s assignment extends the previous weeksâ€™ work by combining dense-vector semantic search with sparse keyword-based filtering. You will build a **hybrid retrieval system** that stores document chunks along with metadata in a SQLite+FAISS index, and performs both FAISS (vector) and full-text keyword search.  The idea is to â€œhave the best of both worldsâ€: exact keyword matches (via SQLite FTS5 or BM25) **and** semantic similarity (via FAISS). In practice, hybrid methods (e.g. weighted score fusion or reciprocal rank fusion) can improve result relevance over using vectors or keywords alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "* **Hybrid Retrieval Techniques:** Understand how to combine dense vector search (FAISS) with sparse keyword search (SQLite FTS5 or BM25). Learn why pure semantic or pure keyword search alone can miss relevant results, and how hybrid search can address both broad â€œvibeâ€ matches and exact queries.\n",
    "\n",
    "* **Metadata and Indexing:** Learn to store document metadata (title, authors, year, keywords, etc.) alongside text chunks and their embeddings. You will design a combined index (SQLite tables + FAISS index) so that each text chunk has associated metadata fields.\n",
    "\n",
    "* **Hybrid Ranking Strategies:** Explore ranking or fusion strategies to merge vector and keyword scores. For example, you might compute a weighted sum of normalized scores, or use **reciprocal rank fusion (RRF)** as a simple ensemble. The goal is to experiment with different score-combination methods and weightings for the final ranking.\n",
    "\n",
    "* **Evaluation (Recall/Hit Rate):** Learn to evaluate retrieval quality. You should measure metrics like **recall** or **hit rate** (the proportion of queries where a relevant doc appears in the top-k results). You will compare the effectiveness of vector-only search, keyword-only search, and the hybrid approach on example queries.\n",
    "\n",
    "## Project Design\n",
    "\n",
    "You will **extend your Week 4 project** by adding metadata storage and keyword search, then implementing a hybrid retrieval pipeline. Key tasks include:\n",
    "\n",
    "* **Index Structure:** Build or extend your SQLite + FAISS index to store *document metadata*, text *chunks*, and their *embeddings*. For example, use a SQLite table `documents(doc_id, title, author, year, keywords, ...)` and an FTS5 table (e.g. `doc_chunks`) that indexes the chunk text. The FAISS index should store the corresponding embeddings (one embedding per chunk) keyed by `doc_id`.\n",
    "\n",
    "* **Keyword Search (FTS5/BM25):** Implement sparse keyword search over the text chunks. The simplest way is to use **SQLite FTS5**: create a virtual FTS table on the chunk text, so that queries like `WHERE doc_chunks MATCH 'term'` return relevant rows. Alternatively, you can use a BM25 library (e.g. Pythonâ€™s `rank_bm25`) to rank chunks by BM25 similarity. Either way, you should be able to retrieve the top-k chunks by exact keyword relevance.\n",
    "\n",
    "* **Hybrid Retrieval:** For a given user query, perform **two separate searches**: one in FAISS (semantic search) and one in the keyword index (FTS5 or BM25). Each returns its own top-k results with scores. You will then *merge* these results. For example, you could normalize the FAISS distances and FTS/BM25 scores, then compute a **weighted sum** or use **reciprocal rank fusion (RRF)** to re-rank the combined set.  The final output should be a single ranked list of document chunks or pages.\n",
    "\n",
    "* **Performance Evaluation:** Design an evaluation to compare methods. For instance, prepare at least **10 test queries** with known relevant documents. Then measure for each method (vector-only, keyword-only, hybrid) how often a relevant document appears in the top-3 (or top-k) results â€” i.e., the hit rate or recall. Report these metrics (e.g. â€œRecall\\@3â€) to see whether hybrid search improves over the baselines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create virtual environment\n",
    "\n",
    "(base) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>conda create -n mod5venv python=3.10.18\n",
    "\n",
    "Activate virtual environment\n",
    "\n",
    "(base) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>conda activate mod5venv\n",
    "\n",
    "Install Pytorch\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "\n",
    "Install faiss-gpu\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>conda install -c pytorch faiss-gpu\n",
    "\n",
    "pysqlite3 is a Python wrapper around the native SQLite C library.\n",
    "To compile it, you need:\n",
    "The SQLite development headers (sqlite3.h)\n",
    "A compiled version of libsqlite3 (or ability to link against it)\n",
    "On Windows, these are not available by default, and the build tools (like MSVC) can't locate them.\n",
    "Even though your system has SQLite (Python includes a built-in sqlite3 module), it uses the embedded version and does not expose the development headers needed to build pysqlite3.\n",
    "\n",
    "The easiest and most reliable fix is to avoid compiling pysqlite3 altogether by using the pre-compiled binary wheel:\n",
    "pip install pysqlite3-binary\n",
    "âœ… This package:\n",
    "Contains the same API as pysqlite3\n",
    "Includes a bundled SQLite3 library\n",
    "Does not require compilation\n",
    "Works out of the box on Windows, macOS, and Linux\n",
    "\n",
    "Despite being a popular package, pysqlite3-binary has no official wheels for some platforms, including: Python 3.10+ on Windows (AMD64) â€” sometimes missing from PyPI pip install pysqlite3-binary so the above command does not work.Insted, we have to use:\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>conda install -c conda-forge pysqlite3\n",
    "\n",
    "Install Other Required Packages\n",
    "\n",
    "BM25 (Best Matching 25) is a ranking function used in information retrieval to estimate the relevance of documents to a search query. It's an improvement over the classic TF-IDF method and is widely used in search engines and question-answering systems.\n",
    "\n",
    "BM25 scores documents based on:\n",
    "\n",
    "Term frequency (how often a query word appears in a document)\n",
    "Inverse document frequency (how rare the word is across all documents)\n",
    "Document length (longer documents are normalized to avoid bias)\n",
    "\n",
    "rank-bm25 is a Python library that implements the BM25 algorithm. It allows you to rank a list of documents based on how relevant they are to a given query.\n",
    "\n",
    "It supports:\n",
    "\n",
    "BM25Okapi: the standard version\n",
    "BM25L and BM25Plus: variants with improved scoring in certain cases\n",
    "\n",
    "jupyter is included to install the Jupyter Notebook interface, which is a popular tool for writing and running Python code interactively.\n",
    "\n",
    "\n",
    "FastAPI is a high-performance framework for building RESTful APIs. Itâ€™s designed to be fast, easy to use, and fully compatible with Python type hints.\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>pip install openai transformers sentence-transformers rank_bm25 python-dotenv jupyter fastapi uvicorn\n",
    "\n",
    "Sentence_transformers may be installed for the second time if it can not be verified.\n",
    "\n",
    "Sentence_transformer is a Python library built on top of Hugging Face's transformers and PyTorch. It makes it easy to generate sentence embeddings using pre-trained models like BERT, RoBERTa, etc., with minimal code.\n",
    "\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>pip install sentence_transformers\n",
    "\n",
    "More packages need to be installed or reinstalled\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>pip install python-dotenv\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>pip install openai\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>pip install langchain\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>conda install -c conda-forge ipywidgets\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>pip install fastapi uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify GPU & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Python Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:42:04) [MSC v.1943 64 bit (AMD64)]\n",
      "âœ… PyTorch Version: 2.5.1\n",
      "âœ… CUDA Available: True\n",
      "âœ… GPU Device: NVIDIA GeForce RTX 4070 SUPER\n",
      "âœ… FAISS GPU Support: True\n",
      "âœ… Loading test model...\n",
      "âœ… Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"âœ… Python Version:\", sys.version)\n",
    "print(\"âœ… PyTorch Version:\", torch.__version__)\n",
    "print(\"âœ… CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"âœ… GPU Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "print(\"âœ… FAISS GPU Support:\", hasattr(faiss, 'GpuIndexFlat'))\n",
    "\n",
    "# Test embedding model\n",
    "print(\"âœ… Loading test model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ… Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .env for api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week5/\n",
    "â”œâ”€â”€ mod5venv/                  # Virtual environment\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â””â”€â”€ documents.jsonl         # Your test documents\n",
    "â”œâ”€â”€ db/\n",
    "â”‚   â””â”€â”€ mydata.db               # SQLite + FTS5 + FAISS\n",
    "â”œâ”€â”€ embeddings/\n",
    "â”‚   â””â”€â”€ faiss_index.bin         # FAISS index\n",
    "â”œâ”€â”€ evaluate.ipynb              # Evaluation notebook\n",
    "â”œâ”€â”€ app.py                      # FastAPI endpoint\n",
    "â””â”€â”€ .env                        # API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 7\n",
      "First chunk: LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queriesâ€”for example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new prompt chains\n",
      "second chunk: can use LangChain components to build new prompt chains or customize existing templates. LangChain also includes components that allow LLMs to access new data sets without retraining.\n",
      "last chunk: and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Read text from a file\n",
    "with open(\"langchain.txt\", \"r\", encoding=\"utf-8\") as file: # langchain is the text file with 2,320 characters\n",
    "    document_text = file.read()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "chunks = splitter.split_text(document_text)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"First chunk: {chunks[0]}\")\n",
    "print(f\"second chunk: {chunks[1]}\")\n",
    "print(f\"last chunk: {chunks[6]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even though 2320 / 512 â‰ˆ 4.5, you get 7 chunks because:\n",
    "\n",
    "* Overlap adds redundancy\n",
    "* Early splits due to text structure reduce average chunk size\n",
    "* Final chunks may be small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7 embeddings.\n",
      "(7, 384)\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight, runs locally\n",
    "embeddings = model.encode(chunks)\n",
    "print(f\"Generated {len(embeddings)} embeddings.\")\n",
    "print(embeddings.shape)  # Output: (3, 384)\n",
    "print(len(embeddings[0]))     # First vector: array of 384 floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceTransformer is the main class in the sentence-transformers library that you use to load and work with models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all-MiniLM-L6-v2 is the name of a pre-trained model specifically designed for generating sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though your text chunks have different lengths (number of characters or words), each one still gets a fixed-size embedding vector of 384 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up SQLite + FTS5 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart the Kernel before run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with auto doc_id = 1\n",
      "All documents: [(1, 'Sample Research Paper')]\n",
      "âœ… Database setup complete.\n"
     ]
    }
   ],
   "source": [
    "# restartt the Kernel before run this cell.\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "\n",
    "# --- ðŸ”¥ DANGER ZONE: Uncomment to DELETE existing DB (for clean dev runs)\n",
    "if os.path.exists(\"db/mydata.db\"):\n",
    "    os.remove(\"db/mydata.db\")\n",
    "conn = sqlite3.connect(\"db/mydata.db\")\n",
    "# --- ðŸ”¥ DANGER ZONE END ---\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(\"db\", exist_ok=True)\n",
    "\n",
    "conn = sqlite3.connect(\"db/mydata.db\")\n",
    "\n",
    "# Example values - replace these with real data or variables from your code\n",
    "# doc_id = 1 # or use a UUID, or autoincremental etc.\n",
    "title = \"Sample Research Paper\"\n",
    "author = \"John Doe\"\n",
    "year = 2025\n",
    "keywords = \"machine learning, NLP, AI\"\n",
    "# chunk_id = 1\n",
    "chunk_text = \"This is a sample chunk of text from the document.\"\n",
    "\n",
    "# Now execute your SQL statements\n",
    "\n",
    "# Metadata table\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        doc_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT,\n",
    "        author TEXT,\n",
    "        year INTEGER,\n",
    "        keywords TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# FTS5 Table for keyword search\n",
    "# FTS: Full-Text Search table  \n",
    "# content: This is the content of the chunk. Indexed for search. \n",
    "# content_rowid: the row ID of the chunk, not indexed for search.\n",
    "# doc_id: Foreign key to documents table. Not indexed for search.\n",
    "conn.execute(\"\"\"\n",
    "    CREATE VIRTUAL TABLE IF NOT EXISTS doc_chunks USING fts5( \n",
    "        content, \n",
    "        content_rowid UNINDEXED,  \n",
    "        doc_id UNINDEXED\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert a document into the metadata table\n",
    "# Use parameterized queries to prevent SQL injection\n",
    "conn.execute(\"\"\"INSERT INTO documents (title, author, year, keywords) VALUES (?, ?, ?, ?)\"\"\",\n",
    "             (title, author, year, keywords))\n",
    "\n",
    "\n",
    "# Get the auto-generated ID\n",
    "# cursor = conn.cursor()\n",
    "# doc_id = cursor.lastrowid\n",
    "# âœ… CORRECT: Get last inserted ID using SQL function\n",
    "doc_id = conn.execute(\"SELECT last_insert_rowid()\").fetchone()[0]\n",
    "print(f\"Inserted document with auto doc_id = {doc_id}\")\n",
    "\n",
    "# Insert chunk for FTS5\n",
    "chunk_id = 1  # This should be unique for each chunk, could be an auto-increment or UUID\n",
    "conn.execute(\"\"\"INSERT INTO doc_chunks(rowid, content, doc_id) VALUES (?, ?, ?)\"\"\",\n",
    "             (chunk_id, chunk_text, doc_id))\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Debug: Check what's in the table --  this is just for verification\n",
    "rows = conn.execute(\"SELECT doc_id, title FROM documents\").fetchall()\n",
    "print(\"All documents:\", rows)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"âœ… Database setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the doc -chunk table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table 'doc_chunks' exists.\n",
      "\n",
      "ðŸ“‹ Table Schema:\n",
      "  (0, 'content', '', 0, None, 0)\n",
      "  (1, 'content_rowid', '', 0, None, 0)\n",
      "  (2, 'doc_id', '', 0, None, 0)\n",
      "\n",
      "ðŸ“¦ Contents of doc_chunks:\n",
      "  Row 1: doc_id=1 â†’ 'This is a sample chunk of text from the document.'\n",
      "\n",
      "ðŸ” Full-text search for 'sample':\n",
      "  Matched: (1, 'This is a sample chunk of text from the document.', 1)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"db/mydata.db\")\n",
    "\n",
    "# === Check if table exists ===\n",
    "tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='doc_chunks';\").fetchall()\n",
    "if not tables:\n",
    "    print(\"âŒ Table 'doc_chunks' does not exist.\")\n",
    "else:\n",
    "    print(\"âœ… Table 'doc_chunks' exists.\")\n",
    "\n",
    "    # === Show schema ===\n",
    "    print(\"\\nðŸ“‹ Table Schema:\")\n",
    "    schema = conn.execute(\"PRAGMA table_info(doc_chunks);\").fetchall()\n",
    "    for col in schema:\n",
    "        print(f\"  {col}\")\n",
    "\n",
    "    # === Show all data ===\n",
    "    print(\"\\nðŸ“¦ Contents of doc_chunks:\")\n",
    "    rows = conn.execute(\"SELECT rowid, content, doc_id FROM doc_chunks\").fetchall()\n",
    "    if rows:\n",
    "        for row in rows:\n",
    "            print(f\"  Row {row[0]}: doc_id={row[2]} â†’ '{row[1]}'\")\n",
    "    else:\n",
    "        print(\"  (No data found)\")\n",
    "\n",
    "    # === Full-text search example ===\n",
    "    print(\"\\nðŸ” Full-text search for 'sample':\")\n",
    "    results = conn.execute(\"\"\"\n",
    "        SELECT rowid, content, doc_id \n",
    "        FROM doc_chunks \n",
    "        WHERE content MATCH 'sample'\n",
    "    \"\"\").fetchall()\n",
    "    for r in results:\n",
    "        print(f\"  Matched: {r}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build FAISS Index ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are from previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n",
      "âœ… FAISS index saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"embeddings\", exist_ok=True) # Create directory if it doesn't exist\n",
    "\n",
    "# Assume 'embeddings' is already defined and is a numpy array of shape (n, d)\n",
    "dimension = embeddings.shape[1] # Get the dimension of the embeddings\n",
    "print(f\"Embedding dimension: {dimension}\")\n",
    "\n",
    "# Create GPU resources\n",
    "res = faiss.StandardGpuResources() # Create resources for GPU index\n",
    "\n",
    "# Build GPU index\n",
    "index_gpu = faiss.GpuIndexFlatL2(res, dimension) \n",
    "# Note: If you want to use CPU, you can use IndexFlatL2 for CPU\n",
    "\n",
    "# Add embeddings\n",
    "index_gpu.add(np.array(embeddings))\n",
    "\n",
    "# ====> Step 1: Convert GPU index to CPU index <====\n",
    "index_cpu = faiss.index_gpu_to_cpu(index_gpu)\n",
    "\n",
    "# ====> Step 2: Save the CPU index\n",
    "faiss.write_index(index_cpu, \"embeddings/faiss_index.bin\")\n",
    "\n",
    "print(\"âœ… FAISS index saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Load Later (Back to GPU if Needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CPU index\n",
    "index_cpu = faiss.read_index(\"embeddings/faiss_index.bin\")\n",
    "\n",
    "# Move index back to GPU (if needed)\n",
    "res = faiss.StandardGpuResources()\n",
    "index_gpu = faiss.index_cpu_to_gpu(res, 0, index_cpu)  # device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, k=3, alpha=0.6): # Hybrid Search Pipeline k=3, alpha=0.6   \n",
    "    # 1. Vector Search (FAISS)  \n",
    "    query_vec = model.encode([query]) # Convert query to embedding\n",
    "    D, I = index.search(np.array(query_vec), k) # Search the index\n",
    "    faiss_results = list(zip(I[0], 1 / (1 + D[0])))  # Convert distance to score\n",
    "\n",
    "    # 2. Keyword Search (FTS5)\n",
    "    cursor = conn.cursor() # Use the existing connection\n",
    "    # Use parameterized query to prevent SQL injection\n",
    "    query = f\"{query}*\"  # FTS5 wildcard search\n",
    "    # Fetch top-k results based on keyword match\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT doc_id, rank FROM doc_chunks\n",
    "        WHERE content MATCH ?\n",
    "        ORDER BY rank\n",
    "        LIMIT ?\"\"\", (query, k)) \n",
    "    keyword_results = [(row[0], row[1]) for row in cursor.fetchall()] # \n",
    "\n",
    "    # Normalize scores (min-max or rank-based)\n",
    "    def normalize_scores(results): \n",
    "        if not results: return {}\n",
    "        scores = {r[0]: r[1] for r in results}\n",
    "        min_s, max_s = min(scores.values()), max(scores.values())\n",
    "        if max_s == min_s:\n",
    "            return {doc: 0.5 for doc in scores}\n",
    "        return {doc: (s - min_s) / (max_s - min_s) for doc, s in scores.items()}\n",
    "\n",
    "    v_scores = normalize_scores(faiss_results)\n",
    "    k_scores = normalize_scores(keyword_results)\n",
    "\n",
    "    # 3. Weighted Fusion\n",
    "    combined = {}\n",
    "    for doc_id in set(v_scores.keys()) | set(k_scores.keys()): \n",
    "        v = v_scores.get(doc_id, 0.0)\n",
    "        k = k_scores.get(doc_id, 0.0)\n",
    "        combined[doc_id] = alpha * v + (1 - alpha) * k\n",
    "\n",
    "    # Sort and return top-k\n",
    "    sorted_docs = sorted(combined.items(), key=lambda x: x[1], reverse=True) \n",
    "    return sorted_docs[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastAPI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fastapi\n",
      "Version: 0.116.1\n",
      "Summary: FastAPI framework, high performance, easy to learn, fast to code, ready for production\n",
      "Home-page: https://github.com/fastapi/fastapi\n",
      "Author: \n",
      "Author-email: =?utf-8?q?Sebasti=C3=A1n_Ram=C3=ADrez?= <tiangolo@gmail.com>\n",
      "License: \n",
      "Location: c:\\users\\ch939\\anaconda3\\envs\\mod5venv\\lib\\site-packages\n",
      "Requires: pydantic, starlette, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show fastapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below command in Anaconda prompt instead.\n",
    "\n",
    "(mod5venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week5>python main.py\n",
    "INFO:     Started server process [14332]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
    "INFO:     127.0.0.1:60387 - \"GET /hybrid_search?query=natural+language+processing HTTP/1.1\" 200 OK\n",
    "INFO:     127.0.0.1:60387 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run the following code in Jupyter Notebook, run it in a separate Python file or terminal.\n",
    "# This is a FastAPI application to expose the hybrid search functionality as an API.\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/hybrid_search\") # Hybrid Search API endpoint\n",
    "async def hybrid_search_api(query: str, k: int = 3): # \n",
    "    results = hybrid_search(query, k)\n",
    "    return {\"results\": [{\"doc_id\": r[0], \"score\": r[1]} for r in results]}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Starter Code Snippets\n",
    "\n",
    "Here are some example code snippets and schemas to help you get started:\n",
    "\n",
    "* **SQLite schema:** Define a table for document metadata and an FTS5 table for text. For example:\n",
    "\n",
    "  ```sql\n",
    "  CREATE TABLE documents (\n",
    "      doc_id    INTEGER PRIMARY KEY,\n",
    "      title     TEXT,\n",
    "      author    TEXT,\n",
    "      year      INTEGER,\n",
    "      keywords  TEXT\n",
    "  );\n",
    "  CREATE VIRTUAL TABLE doc_chunks USING fts5(\n",
    "      content,                      -- chunk text\n",
    "      content='documents',          -- external content table\n",
    "      content_rowid='doc_id'        -- link to documents.doc_id\n",
    "  );\n",
    "  ```\n",
    "\n",
    "  This creates an FTS5 index on the `content` column (chunk text) referencing the `documents` table.\n",
    "\n",
    "* **Inserting data:** Insert your documents and chunk text. For example, in Python:\n",
    "\n",
    "  ```python\n",
    "  conn = sqlite3.connect(\"mydata.db\")\n",
    "  # Insert document metadata\n",
    "  conn.execute(\"INSERT INTO documents VALUES (?, ?, ?, ?, ?)\",\n",
    "               (doc_id, title, author, year, keywords))\n",
    "  # Insert chunk text into FTS table, linking by rowid\n",
    "  conn.execute(\"INSERT INTO doc_chunks(rowid, content) VALUES (?, ?)\",\n",
    "               (doc_id, chunk_text))\n",
    "  conn.commit()\n",
    "  ```\n",
    "\n",
    "  Or in raw SQL, you might SELECT from a content table into the FTS table as shown in.\n",
    "\n",
    "* **Keyword query (FTS5):** A full-text query can be written as:\n",
    "\n",
    "  ```sql\n",
    "  SELECT doc_id, title\n",
    "  FROM documents\n",
    "  JOIN doc_chunks ON documents.doc_id = doc_chunks.rowid\n",
    "  WHERE doc_chunks MATCH 'search terms'\n",
    "  LIMIT 5;\n",
    "  ```\n",
    "\n",
    "  This returns documents whose chunks match the query terms.\n",
    "\n",
    "* **BM25 example (Python):** If you use `rank_bm25`, example usage is:\n",
    "\n",
    "  ```python\n",
    "  from rank_bm25 import BM25Okapi\n",
    "  docs = [\"text of doc1 ...\", \"text of doc2 ...\", ...]\n",
    "  tokenized_docs = [doc.split() for doc in docs]\n",
    "  bm25 = BM25Okapi(tokenized_docs)\n",
    "  query = \"example query\"\n",
    "  tokenized_query = query.split()\n",
    "  top_docs = bm25.get_top_n(tokenized_query, docs, n=3)\n",
    "  ```\n",
    "\n",
    "  This returns the top 3 documents by BM25 score.\n",
    "\n",
    "* **Hybrid score merging:** Hereâ€™s a simple Python example of a weighted-sum merger:\n",
    "\n",
    "  ```python\n",
    "  def hybrid_score(vec_score, key_score, alpha=0.5):\n",
    "      # Assume vec_score and key_score are normalized (0-1).\n",
    "      return alpha * vec_score + (1 - alpha) * key_score\n",
    "\n",
    "  # Example usage for re-ranking top results:\n",
    "  combined = []\n",
    "  for doc, v_score in faiss_results:\n",
    "      k_score = keyword_scores.get(doc, 0.0)\n",
    "      combined_score = hybrid_score(v_score, k_score, alpha=0.6)\n",
    "      combined.append((doc, combined_score))\n",
    "  combined.sort(key=lambda x: x[1], reverse=True)\n",
    "  top_k = combined[:3]\n",
    "  ```\n",
    "\n",
    "  You can adjust `alpha` or use other formulas (e.g. max, reciprocal rank fusion).\n",
    "\n",
    "* **FastAPI route skeleton:** To serve hybrid search via an API, you might write:\n",
    "\n",
    "  ```python\n",
    "  from fastapi import FastAPI\n",
    "  app = FastAPI()\n",
    "\n",
    "  @app.get(\"/hybrid_search\")\n",
    "  async def hybrid_search(query: str, k: int = 3):\n",
    "      # 1. Compute query embedding for FAISS\n",
    "      # 2. Get top-k from FAISS and top-k from SQLite FTS/BM25\n",
    "      # 3. Merge scores (as above) and select final top-k documents\n",
    "      return {\"results\": top_k_results}\n",
    "  ```\n",
    "\n",
    "  This endpoint takes a `query` string and returns the top-k hybrid results in JSON.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables\n",
    "\n",
    "Your submission should include:\n",
    "\n",
    "* The **updated SQLite+FAISS index** (or code to build it) that contains the document chunks, embeddings, and metadata.\n",
    "* The **hybrid retrieval pipeline code**, including FAISS search, FTS/BM25 search, and the score-merging logic.\n",
    "* An **evaluation notebook** (e.g. Jupyter) showing at least 10 example queries and reporting metrics (e.g. recall or hit rate @3) for vector-only, keyword-only, and hybrid search.\n",
    "* A **FastAPI endpoint** implementation (`/hybrid_search`) that returns the hybrid top-3 results for a given query (as JSON).\n",
    "\n",
    "Include comments in your code to explain each step. Your evaluation should show whether the hybrid method improves over using vectors or keywords alone.\n",
    "\n",
    "\n",
    "**References:** This assignment is based on standard practices for hybrid search using FAISS and SQLite FTS5. Reciprocal rank fusion and other fusion methods are known techniques in information retrieval. For more background on hit rate metrics, see analytics tutorials on search evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mod5venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
