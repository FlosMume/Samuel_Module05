{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "03469d38",
      "metadata": {},
      "source": [
        "# üß† Week 5: Supervised Finetuning (SFT) - I\n",
        "**Theme:** Teaching LLMs to Follow Instructions  \n",
        "**Project:** LoRA vs. Full Finetuning on HuggingFace with Deepspeed/TRL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1efeca09",
      "metadata": {},
      "source": [
        "## üìò 1. What is Supervised Finetuning (SFT)?\n",
        "SFT = Pretrained model + Instruction-following data ‚Üí Task-specific model\n",
        "\n",
        "**Supervised Fine-Tuning (SFT)** is the process of further training a pre-trained language model on a labeled dataset to specialize it for specific tasks or domains.\n",
        "\n",
        "**Key points:**\n",
        "- Builds on top of a pretrained model like LLaMA, GPT, or Mistral.\n",
        "- Uses instruction-response pairs (like question-answer).\n",
        "- Enhances instruction-following ability.\n",
        "- It's a middle stage between pretraining and alignment (e.g., RLHF).\n",
        "\n",
        "**SFT Pipeline:**\n",
        "1. Pretrained Model\n",
        "2. Supervised Dataset\n",
        "3. Fine-tuned Instruction Model\n",
        "\n",
        "\n",
        "**Example:**\n",
        "| Before SFT                 | After SFT                          |\n",
        "|---------------------------|------------------------------------|\n",
        "| Random generic responses  | Follows user instructions clearly |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580db05c",
      "metadata": {},
      "source": [
        "## üìä 2. How to Get SFT Data\n",
        "\n",
        "**4 Types of Data Sources:**\n",
        "1. **Manual Curation**: Human-created prompts and responses.\n",
        "2. **AI-Generated**: Use GPT models to self-generate instruction data.\n",
        "3. **Open Datasets**: Alpaca, OASST1, Dolly, HH-RLHF, etc.\n",
        "4. **Data Augmentation**: Rephrasing, adding context, changing perspective.\n",
        "\n",
        "**Goal**: Create high-quality, diverse, and instruction-aligned examples.\n",
        "\n",
        "* here we use the second way to generate our data using openAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5634878",
      "metadata": {},
      "source": [
        "Create an virtual environment\n",
        "\n",
        "(base) C:\\Users\\ch939>conda create -n sft_env python = 3.10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b5a55e",
      "metadata": {},
      "source": [
        "Activate the virtual environment\n",
        "\n",
        "(base) C:\\Users\\ch939>conda activate sft_env"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca4e3ee",
      "metadata": {},
      "source": [
        "Now, choose the kernel to be sft_env(python3.10.18)\n",
        "\n",
        "In the Anaconda Prompt, we can see:\n",
        "\n",
        "(sft_env) C:\\Users\\ch939>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b9fdc31",
      "metadata": {},
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0093704b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 channel Terms of Service accepted\n",
            "Channels:\n",
            " - conda-forge\n",
            " - pytorch\n",
            " - defaults\n",
            "Platform: win-64\n",
            "Collecting package metadata (repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: C:\\Users\\ch939\\anaconda3\\envs\\sft_env\n",
            "\n",
            "  added / updated specs:\n",
            "    - openai\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    distro-1.9.0               |     pyhd8ed1ab_1          41 KB  conda-forge\n",
            "    jiter-0.10.0               |  py310hc226416_0         178 KB  conda-forge\n",
            "    openai-1.99.9              |     pyhd8ed1ab_0         303 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         522 KB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  annotated-types    conda-forge/noarch::annotated-types-0.7.0-pyhd8ed1ab_1 \n",
            "  anyio              conda-forge/noarch::anyio-4.10.0-pyhe01879c_0 \n",
            "  certifi            conda-forge/noarch::certifi-2025.8.3-pyhd8ed1ab_0 \n",
            "  distro             conda-forge/noarch::distro-1.9.0-pyhd8ed1ab_1 \n",
            "  h11                conda-forge/noarch::h11-0.16.0-pyhd8ed1ab_0 \n",
            "  h2                 conda-forge/noarch::h2-4.2.0-pyhd8ed1ab_0 \n",
            "  hpack              conda-forge/noarch::hpack-4.1.0-pyhd8ed1ab_0 \n",
            "  httpcore           conda-forge/noarch::httpcore-1.0.9-pyh29332c3_0 \n",
            "  httpx              conda-forge/noarch::httpx-0.28.1-pyhd8ed1ab_0 \n",
            "  hyperframe         conda-forge/noarch::hyperframe-6.1.0-pyhd8ed1ab_0 \n",
            "  idna               conda-forge/noarch::idna-3.10-pyhd8ed1ab_1 \n",
            "  jiter              conda-forge/win-64::jiter-0.10.0-py310hc226416_0 \n",
            "  openai             conda-forge/noarch::openai-1.99.9-pyhd8ed1ab_0 \n",
            "  pydantic           conda-forge/noarch::pydantic-2.11.7-pyh3cfb1c2_0 \n",
            "  pydantic-core      conda-forge/win-64::pydantic-core-2.33.2-py310hed05c55_0 \n",
            "  sniffio            conda-forge/noarch::sniffio-1.3.1-pyhd8ed1ab_1 \n",
            "  tqdm               conda-forge/noarch::tqdm-4.67.1-pyhd8ed1ab_1 \n",
            "  typing-extensions  conda-forge/noarch::typing-extensions-4.14.1-h4440ef1_0 \n",
            "  typing-inspection  conda-forge/noarch::typing-inspection-0.4.1-pyhd8ed1ab_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages: ...working...\n",
            "openai-1.99.9        | 303 KB    |            |   0% \n",
            "\n",
            "jiter-0.10.0         | 178 KB    |            |   0% \u001b[A\n",
            "\n",
            "\n",
            "distro-1.9.0         | 41 KB     |            |   0% \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "distro-1.9.0         | 41 KB     | ###9       |  39% \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "distro-1.9.0         | 41 KB     | ########## | 100% \u001b[A\u001b[A\n",
            "\n",
            "jiter-0.10.0         | 178 KB    | 8          |   9% \u001b[A\n",
            "openai-1.99.9        | 303 KB    | 5          |   5% \n",
            "\n",
            "jiter-0.10.0         | 178 KB    | ########## | 100% \u001b[A\n",
            "\n",
            "jiter-0.10.0         | 178 KB    | ########## | 100% \u001b[A\n",
            "\n",
            "\n",
            "distro-1.9.0         | 41 KB     | ########## | 100% \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "distro-1.9.0         | 41 KB     | ########## | 100% \u001b[A\u001b[A\n",
            "openai-1.99.9        | 303 KB    | ########## | 100% \n",
            "openai-1.99.9        | 303 KB    | ########## | 100% \n",
            "openai-1.99.9        | 303 KB    | ########## | 100% \n",
            "                                                     \n",
            "\n",
            "\n",
            "                                                     \u001b[A\n",
            "\n",
            "\n",
            "                                                     \u001b[A\u001b[A done\n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "! conda install openai -y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01edb39e",
      "metadata": {},
      "source": [
        "Copy the .env file to the working directory and install python-dotenv for the retrieval of the api keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8ffc905e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c164ccb",
      "metadata": {},
      "source": [
        "Verify that dotenv has been installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "99f6bb10",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\dotenv\\__init__.py\n"
          ]
        }
      ],
      "source": [
        "import dotenv\n",
        "print(dotenv.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "65cace7c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'question': 'Can you explain the difference between a stack and a queue?',\n",
              "  'answer': 'A stack is a data structure that follows the Last In First Out (LIFO) principle, meaning that the last element added to the stack is the first one to be removed. Conversely, a queue operates on a First In First Out (FIFO) basis, where the first element added is the first one to be removed. This means that stacks are more suitable for scenarios like function call management, while queues are ideal for scheduling tasks.'},\n",
              " {'question': 'What is the purpose of version control systems like Git?',\n",
              "  'answer': 'Version control systems like Git are used to track changes in source code over time. They allow multiple developers to collaborate on a project efficiently, manage different versions of code, revert to previous versions if needed, and maintain a history of changes. Git specifically enables branching and merging, which helps in developing features in isolation before integrating them into the main codebase.'}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def get_ai_generated_data():\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        print(\"‚ö†Ô∏è No OpenAI API key - using placeholder data\")\n",
        "        return [{\"instruction\": \"What are your technical skills?\", \"response\": \"Python, data analysis\"}]\n",
        "\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    prompt = \"Create 2 interview Q&A pairs for a software developer in JSON format. Output only JSON.\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    # If model returns Markdown-style JSON block\n",
        "    if \"```json\" in content:\n",
        "        content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "    elif \"```\" in content:\n",
        "        content = content.split(\"```\")[1]\n",
        "\n",
        "    try:\n",
        "        data = json.loads(content)\n",
        "        if isinstance(data, dict):\n",
        "            return data.get(\"examples\", [])\n",
        "        elif isinstance(data, list):\n",
        "            return data\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Unexpected JSON format:\", type(data))\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to parse JSON:\", e)\n",
        "        print(\"Raw content:\", content)\n",
        "        return []\n",
        "\n",
        "# Example call\n",
        "get_ai_generated_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a93113",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e594dbc9",
      "metadata": {},
      "source": [
        "## üß© 3. Formatting: ChatML\n",
        "**ChatML** is a structured dialogue format used to simulate role-based conversations during SFT training.\n",
        "\n",
        "**Structure:**\n",
        "```\n",
        "<|im_start|>user\n",
        "What's the capital of France?\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Paris.\n",
        "<|im_end|>\n",
        "```\n",
        "\n",
        "**Why it matters:**\n",
        "- Improves consistency\n",
        "- Helps multi-turn dialogue modeling\n",
        "- Matches formatting expectations for LLaMA and OpenAI-style models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a7e581",
      "metadata": {},
      "source": [
        "## üîç 4. Full Finetune vs. LoRA\n",
        "\n",
        "\n",
        "| Aspect               | Full Fine-Tuning      | LoRA (Low-Rank Adaptation) |\n",
        "|----------------------|-----------------------|-----------------------------|\n",
        "| Trainable Params     | 100%                  | ~0.5‚Äì1%                     |\n",
        "| Memory Usage         | Very High             | Low                         |\n",
        "| Flexibility          | Maximum               | Good for most tasks         |\n",
        "| Training Time        | Longer                | Faster                      |\n",
        "| Use Case             | Critical domain shift | Resource-efficient tuning   |\n",
        "\n",
        "**Recommendation**: Use LoRA for most educational and practical settings unless full retraining is justified.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c22009",
      "metadata": {},
      "source": [
        "The below installation of PyTorch with CUDA has to be done in Anaconda Prompt. It takes 15 minutes. It is not working using the below command using python environment.\n",
        "\n",
        "!conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a694d06f",
      "metadata": {},
      "source": [
        "Execute the below in Anaconda Prompt\n",
        "\n",
        "conda install numpy --force-reinstall --yes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd1e3be",
      "metadata": {},
      "source": [
        "Reinstall pytorch and transformers to fix any broken dependencies\n",
        "\n",
        "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia --force-reinstall --yes\n",
        "\n",
        "pip install --force-reinstall transformers peft accelerate datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef2b0e5",
      "metadata": {},
      "source": [
        "There were errors. Rework is needed as follows:\n",
        "\n",
        "\n",
        "conda deactivate\n",
        "conda env remove -n sft_env\n",
        "conda create -n sft_env python=3.10\n",
        "conda activate sft_env\n",
        "\n",
        "# Install matching torch stack\n",
        "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y\n",
        "\n",
        "# Install numpy via conda (critical for Windows)\n",
        "conda install numpy -y\n",
        "\n",
        "# Install Hugging Face stack via pip\n",
        "pip install transformers peft accelerate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2c05b584",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1\n",
            "CUDA available: True\n",
            "fsspec version: 2025.3.0\n",
            "‚ùå Error: There was a specific connection error when trying to load gpt2:\n",
            "401 Client Error: Unauthorized for url: https://huggingface.co/gpt2/resolve/main/config.json (Request ID: Root=1-68a2bb82-758a71b036d37c7903d168eb;50198f83-f8f6-49c5-a561-c5f9a6be3b34)\n",
            "\n",
            "Invalid credentials in Authorization header\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import fsspec\n",
        "\n",
        "print('PyTorch version:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "print('fsspec version:', fsspec.__version__)\n",
        "\n",
        "# Optional: test transformers\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "    print('‚úÖ All good!')\n",
        "except Exception as e:\n",
        "    print('‚ùå Error:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2feb4e17",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "If you're just loading public models like gpt2, you don't need to be logged in to Hugging Face.\n",
        "\n",
        "\n",
        "Ihuggingface-cli logout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4b949c9e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA: True\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b2d1bb0a7fd4e338a52babdae12ce1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ch939\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1aaa316af2664978a2df963ca3338f13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4f93bbcf74946789c80c7dbd04703de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81ce30694bd84f27a633945984d602c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb26d93f0f0b46f09ff8c8a8f27721ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64468e04ea384258823bfe4147d44ba9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93e175bdac004705b45c27a737795dd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model device: cpu\n",
            "‚úÖ Success!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"Model device:\", model.device)\n",
        "print(\"‚úÖ Success!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1dd9c3b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\transformers\\__init__.py\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9205d492",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c738c0d4f8c4c1bb4275f9f6b62f325",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ch939\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "252cea86bfbd45fc841198403ac7021d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d714476805fc414cbb7cf9af286efb4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e72b8244a4854670972d57e9a3b880e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4bab66ef6084886ac5ac6cc09961626",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "992f5a6a48f8482e8c9a5d860a20d8ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:2156: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "model_name = \"microsoft/DialoGPT-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"]\n",
        ")\n",
        "lora_model = get_peft_model(base_model, lora_config)\n",
        "lora_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bafd91d5",
      "metadata": {},
      "source": [
        "You're seeing several warnings and messages during model loading ‚Äî but the good news is: your code is working! "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0a49967",
      "metadata": {},
      "source": [
        "CUDA is working in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae9d4cc",
      "metadata": {},
      "source": [
        "## ‚ö° 5. DeepSpeed\n",
        "\n",
        "**DeepSpeed** is a library from Microsoft that allows efficient distributed training of large models.\n",
        "\n",
        "**Modes:**\n",
        "- **ZeRO-1/2/3** for optimizer/shard parallelism.\n",
        "- **CPU Offload** to reduce GPU memory usage.\n",
        "- **Mixed Precision** for speed and efficiency.\n",
        "\n",
        "**Best for**: Scaling training to large models like 13B+, saving memory, or training on multiple GPUs.\n",
        "\n",
        "Enables memory-efficient training. Example config:\n",
        "```json\n",
        "{\n",
        "  \"zero_optimization\": {\"stage\": 2},\n",
        "  \"fp16\": {\"enabled\": true}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ed379cb",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è 6. TRL Package (SFTTrainer)\n",
        "\n",
        "**Transformers Reinforcement Learning (TRL)** by Hugging Face includes:\n",
        "\n",
        "- `SFTTrainer`: Simplified supervised training loop.\n",
        "- `PPOTrainer`: RLHF with Proximal Policy Optimization.\n",
        "- `DPOTrainer`: Direct Preference Optimization.\n",
        "- `RewardTrainer`: For reward model training.\n",
        "\n",
        "**Why TRL?**\n",
        "- Abstracts away complex setup.\n",
        "- Faster experimentation.\n",
        "- Supports all major fine-tuning and alignment workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ef66f2",
      "metadata": {},
      "source": [
        "Install the trl Package\n",
        "\n",
        "pip install trl\n",
        "\n",
        "Verify Installation\n",
        "\n",
        "python -c \"from trl import SFTTrainer; print('trl is installed successfully')\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5ef5ffff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\peft\\mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ch939\\anaconda3\\envs\\sft_env\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fab9029498dd4ac7a89f1f1f6556f301",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying formatting function to train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2a54726b0684ca7abf47ee31784c177",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fe3065754bd466eb386f3e5395f7882",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b90ecc1d8b004330894da8ae7676b998",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.428500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.914400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# for CUDA mechain run following \n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "\n",
        "# ‚úÖ --- 1.Prepare the dataset (must have a 'text' field)\n",
        "demo_dataset = Dataset.from_list([\n",
        "    {\"text\": \"Human: What is Python?\\nAssistant: Python is a programming language.\"},\n",
        "    {\"text\": \"Human: How do I learn coding?\\nAssistant: Start with basic concepts and practice regularly.\"}\n",
        "])\n",
        "\n",
        "# ‚úÖ --- 2. Load Base Model and Tokenizer ---\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small, fast, public\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token to eos_token if not defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    # Add device_map=\"auto\" if using GPU and want to leverage accelerate\n",
        "    # device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# ‚úÖ --- 3. Define LoRA Configuration ---\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                     # Rank of LoRA approximation\n",
        "    lora_alpha=16,           # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Try to target attention layers\n",
        "    lora_dropout=0.05,       # Dropout for LoRA layers\n",
        "    bias=\"none\",             # No bias in LoRA\n",
        "    task_type=\"CAUSAL_LM\"    # For language modeling\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Optional: Print how many parameters are being trained\n",
        "lora_model.print_trainable_parameters()  # Should show small % (e.g., ~0.1%)\n",
        "\n",
        "\n",
        "# ‚úÖ --- 4. Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./trl_sft_demo\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=5e-4,\n",
        "    logging_steps=1,\n",
        "    save_steps=10,\n",
        "    save_total_limit=1,\n",
        "    fp16=True, # Set to True if using GPU and CUDA\n",
        "    optim=\"adamw_torch\",  # Good default\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "def formatting_func(example):\n",
        "    return example[\"text\"]\n",
        "\n",
        "# ‚úÖ --- 5. Initialize SFTTrainer ‚Äî no config, no extras\n",
        "trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=demo_dataset,\n",
        "    processing_class=tokenizer, # tokenizer=tokenizer, this statement is outdated\n",
        "    # dataset_text_field=\"text\",  # Required by SFTTrainer, # this statement is outdated\n",
        "    formatting_func=formatting_func,        # ‚úÖ Use this instead\n",
        "    # max_seq_length=512,         # Optional: set sequence length   # this statement is to be removed.\n",
        "    peft_config=lora_config, # newly added\n",
        ")\n",
        "\n",
        "# ‚úÖ --- 6. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# ‚úÖ --- 7. Save LoRA Adapter (Optional) ---\n",
        "trainer.save_model(\"./sft_finetuned_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a758c79b",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91de6d6c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "47e09efc",
      "metadata": {},
      "source": [
        "For diagnosis: find that tokenizer=tokenizer, this statement is outdated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "85da9f42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRL version: 0.21.0\n",
            "SFTTrainer.__init__ parameters: ['self', 'model', 'args', 'data_collator', 'train_dataset', 'eval_dataset', 'processing_class', 'compute_loss_func', 'compute_metrics', 'callbacks', 'optimizers', 'optimizer_cls_and_kwargs', 'preprocess_logits_for_metrics', 'peft_config', 'formatting_func']\n",
            "‚ùå 'tokenizer' is NOT in the signature ‚Äî something is wrong\n"
          ]
        }
      ],
      "source": [
        "# save as diagnose_trl.py or run in prompt\n",
        "import trl\n",
        "from trl import SFTTrainer\n",
        "from inspect import signature\n",
        "\n",
        "print(f\"TRL version: {trl.__version__}\")\n",
        "\n",
        "# Print all __init__ parameters of SFTTrainer\n",
        "init_params = list(signature(SFTTrainer.__init__).parameters.keys())\n",
        "print(f\"SFTTrainer.__init__ parameters: {init_params}\")\n",
        "\n",
        "# Check for 'tokenizer'\n",
        "if 'tokenizer' in init_params:\n",
        "    print(\"‚úÖ 'tokenizer' is supported!\")\n",
        "else:\n",
        "    print(\"‚ùå 'tokenizer' is NOT in the signature ‚Äî something is wrong\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a99c70d",
      "metadata": {},
      "source": [
        "Explanation:\n",
        "1. global_step=2\n",
        "\n",
        "    This means the training process completed 2 optimization steps (i.e., two batches were processed and used to update the model's parameters).\n",
        "\n",
        "2. training_loss=9.432311534881592\n",
        "\n",
        "    This is the final training loss averaged over the training steps. A higher loss suggests the model hasn't learned much yet, likely because:\n",
        "        * It‚Äôs early in training (only 2 steps).\n",
        "        * The model needs more tuning or a better learning rate.\n",
        "        * The data might be complex or noisy.\n",
        "\n",
        "3. train_runtime=2.921\n",
        "\n",
        "    Total time in seconds the training took ‚Äî in this case, around 2.9 seconds.\n",
        "\n",
        "4. train_samples_per_second=0.685\n",
        "\n",
        "    The average number of training examples processed per second. Since only 2 steps were taken, the dataset or batch size may have been small.\n",
        "\n",
        "5. train_steps_per_second=0.685\n",
        "\n",
        "    How many steps (i.e., parameter updates) were completed per second. It matches the sample rate, implying 1 sample per step.\n",
        "\n",
        "6. total_flos=18722451456.0\n",
        "\n",
        "    The total number of floating point operations (FLOPs) executed during training. It's a proxy for how computationally intensive the training was.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f662325",
      "metadata": {},
      "source": [
        "### Explanation of Each Metric during model training:\n",
        "- ‚úÖ loss\n",
        "    *What it is*: Measures the model's prediction error ‚Äî how far off the model is from the target output.\n",
        "\n",
        "    *What to look for*: We want this to decrease over time.\n",
        "\n",
        "    A value of 0.1097 or 0.1366 is relatively low, which is promising, assuming it continues trending downward.\n",
        "\n",
        "    Temporary small increases (like from 0.1097 to 0.1366) can happen due to learning rate fluctuations or noisy batches.\n",
        "\n",
        "- ‚úÖ grad_norm (Gradient Norm)\n",
        "    *What it is*: L2 norm of the gradients ‚Äî essentially, how large the updates to the model's weights are.\n",
        "\n",
        "    *What to look for*:\n",
        "\n",
        "    If this value is too large, it may indicate exploding gradients.\n",
        "\n",
        "    If too small (near zero), it may mean vanishing gradients or that training is plateauing.\n",
        "\n",
        "    for example:\n",
        "\n",
        "    0.969 ‚Üí healthy magnitude, meaning the model is still learning.\n",
        "\n",
        "    0.278 ‚Üí much smaller, which could mean learning is slowing down ‚Äî possibly nearing convergence, or may need LR adjustment.\n",
        "\n",
        "- ‚úÖ learning_rate\n",
        "    *What it is*: The rate at which the model updates its weights. Often decays over time (e.g., cosine scheduler).\n",
        "\n",
        "    *What to look for*:\n",
        "\n",
        "    A decaying learning rate is common and helps fine-tune the model toward convergence.\n",
        "\n",
        "    0.000126 ‚Üí slightly higher; 0.000120 ‚Üí lower. This drop suggests a learning rate schedule is being applied, as expected.\n",
        "\n",
        "- ‚úÖ epoch\n",
        "    *What it is*: Indicates how far along you are in training (e.g., 4.67 = 67% through the 5th epoch).\n",
        "\n",
        "    *What to look for*: Helps track progression. You‚Äôd want to compare loss and grad_norm across epochs to evaluate learning trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb59b478",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4094f035",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary\n",
        "\n",
        "You‚Äôve learned:\n",
        "- What SFT is and why it‚Äôs essential\n",
        "- Where and how to get quality data\n",
        "- How to use ChatML format\n",
        "- When to choose LoRA vs full tuning\n",
        "- How to leverage DeepSpeed and TRL for scale and alignment\n",
        "\n",
        "## - For the full llama3 sft code, check out class_5_llama3.py"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sft_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
